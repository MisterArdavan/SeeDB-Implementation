{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JyCTH6bNwcMd"
   },
   "outputs": [],
   "source": [
    "# dependencies and libraries\n",
    "#%load_ext lab_black\n",
    "import sqlite3\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from decimal import Decimal\n",
    "from scipy.stats import wasserstein_distance, entropy\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import math, heapq\n",
    "import timeit\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8yY5YHCwKL0"
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"SeeDb.db\")\n",
    "cursor = conn.cursor()\n",
    "agg_funcs = [\"avg\", \"sum\", \"count\", \"min\", \"max\"]\n",
    "dimensions = [\n",
    "    \"sex\",\n",
    "    \"work_class\",\n",
    "    \"education\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"native_country\",\n",
    "    \"class\",\n",
    "]  # marital_status should not be included\n",
    "measures = [\n",
    "    \"age\",\n",
    "    # \"fnlwgt\",\n",
    "    \"education_num\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "]\n",
    "epsilon = Decimal(1e-8)\n",
    "n_partitions = 10\n",
    "S_row = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nCZ0x4lfDg1"
   },
   "outputs": [],
   "source": [
    "# this cell has the functions definitions responsible to create, fill and query the database\n",
    "# run this cell once whenever the notebook is disconnected to create the database and fill it locally\n",
    "def create_fill_database():\n",
    "    # Drop table if it exists\n",
    "    cursor.execute(\"\"\"DROP TABLE IF EXISTS census\"\"\")\n",
    "    # Create table\n",
    "    cursor.execute(\n",
    "        \"\"\"CREATE TABLE census\n",
    "                (age int, work_class text, fnlwgt int, education text, education_num int, marital_status text, occupation text, relationship text, \n",
    "                race text, sex text, capital_gain int, capital_loss int, hours_per_week int, native_country text, class text)\"\"\"\n",
    "    )\n",
    "\n",
    "    # fill the database\n",
    "    df = pd.DataFrame(pd.read_csv(\"adultdata.csv\"))\n",
    "    for row in df.itertuples():\n",
    "        cursor.execute(\n",
    "            \"\"\"\n",
    "                  INSERT INTO census (age, work_class, fnlwgt, education, education_num, marital_status, occupation, relationship, \n",
    "                  race, sex, capital_gain, capital_loss, hours_per_week, native_country, class)\n",
    "                  VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "                  \"\"\",\n",
    "            (\n",
    "                row.AGE,\n",
    "                row.WORK_CLASS,\n",
    "                row.FNLWGT,\n",
    "                row.EDUCATION,\n",
    "                row.EDUCATION_NUM,\n",
    "                row.MARITAL_STATUS,\n",
    "                row.OCCUPATION,\n",
    "                row.RELATIONSHIP,\n",
    "                row.RACE,\n",
    "                row.SEX,\n",
    "                row.CAPITAL_GAIN,\n",
    "                row.CAPITAL_LOSS,\n",
    "                row.HOURS_PER_WEEK,\n",
    "                row.NATIVE_COUNTRY,\n",
    "                row.CLASS,\n",
    "            ),\n",
    "        )\n",
    "    conn.commit()\n",
    "    # conn.close()\n",
    "\n",
    "\n",
    "# create_fill_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enWr0ndKIvSI"
   },
   "source": [
    "## Phase-based Execution Framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4I5tL89f2mKi"
   },
   "outputs": [],
   "source": [
    "def exec_view(view):\n",
    "    \"\"\"\n",
    "    Returns the result of running the view query for target and reference\"\"\"\n",
    "    group_by, measurement, agg_func = view\n",
    "    target_query = \"select {0}, {1}({2}) from census where marital_status in ('Married-AF-spouse', 'Married-civ-spouse', 'Married-spouse-absent') Group By {0};\".format(\n",
    "        group_by, agg_func, measurement\n",
    "    )\n",
    "    ref_query = \"select {0}, {1}({2}) from census where marital_status in ('Never-married', 'Widowed','Divorced', 'Separated') Group By {0};\".format(\n",
    "        group_by, agg_func, measurement\n",
    "    )\n",
    "    target_df = execute_query(target_query)\n",
    "    ref_df = execute_query(ref_query)\n",
    "    return target_df, ref_df\n",
    "\n",
    "\n",
    "def execute_query(query):\n",
    "    \"\"\"Returns a dataframe object with all the resulting records\n",
    "    Arguments:\n",
    "      query: The query to execute\"\"\"\n",
    "    records_df = pd.read_sql_query(query, conn)\n",
    "    return records_df\n",
    "\n",
    "\n",
    "def partition_DB(n):\n",
    "    \"\"\"Returns a list of each partition starting point and a -1 at the end\n",
    "    Arguments:\n",
    "      n: number of partitions\n",
    "    \"\"\"\n",
    "    records_count = execute_query(\"select count(*) from census\").iloc[0]\n",
    "    partition_size = int(records_count / n)\n",
    "    range_starts = []\n",
    "    for i in range(n):\n",
    "        range_starts.append(i * partition_size)\n",
    "    range_starts.append(-1)\n",
    "    return range_starts\n",
    "\n",
    "\n",
    "def run_query_on_partition(query, start, end=-1):\n",
    "    \"\"\"Execute query on dataset partition\n",
    "    Arguments:\n",
    "      query: query to execute\n",
    "      start: start of the dataset partition (not inclusive)\n",
    "      end: end of the dataset partition (inclusive) (default -1 for the last partition)\n",
    "    Returns:\n",
    "      Dataframe containing result of executing the query on the dataset partition\n",
    "    \"\"\"\n",
    "    part1 = query\n",
    "    part2 = \"\"\n",
    "    idx = query.lower().find(\"group by\")\n",
    "    if idx != -1:\n",
    "        part1 = query[:idx]\n",
    "        part2 = query[idx:]\n",
    "    # add the partitioning condition to the where clause\n",
    "    if \"where\" in part1.lower():\n",
    "        part1 += \" AND rowid > \" + str(start)\n",
    "    else:\n",
    "        part1 += \" where rowid > \" + str(start)\n",
    "    # the last partition doesn't have an end condition\n",
    "    if end != -1:\n",
    "        part1 += \" AND rowid <= \" + str(end)\n",
    "\n",
    "    return execute_query(part1 + \" \" + part2)\n",
    "\n",
    "\n",
    "print(\"Partitioning output on census database: \")\n",
    "print(partition_DB(n_partitions))\n",
    "print(\"run_query_on_partition output: \")\n",
    "display(run_query_on_partition(\"select * from census\", 0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AN7ZuEB94PvM"
   },
   "source": [
    "## Sharing-based Optimizations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnRDlvlCPeV8"
   },
   "source": [
    "Implementation of First-Fit Decreasing Algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: we have to try out different values of S_row using a grid search to see which runs faster compared to when we don't combine group-by attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVR7X8B6ifCY"
   },
   "outputs": [],
   "source": [
    "def get_all_views():\n",
    "    \"\"\"Returns all possible a * m * f views\"\"\"\n",
    "    views = []\n",
    "    for f in agg_funcs:\n",
    "        for a in dimensions:\n",
    "            for m in measures:\n",
    "                views.append((a, m, f))\n",
    "    return views\n",
    "\n",
    "\n",
    "v = get_all_views()\n",
    "print(\"Total number of views: {}. Sample view: {}\".format(len(v), v[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2skrhNBj4Uhc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_views_with_same_group_by(views):\n",
    "    \"\"\"Returns a dictionary, the key is the group by attribute and the value is the view tuple\"\"\"\n",
    "    d = defaultdict(list)\n",
    "\n",
    "    for view in views:\n",
    "        (a, m, f) = view\n",
    "        d[a].append(view)\n",
    "    return d\n",
    "\n",
    "\n",
    "# print(len(get_views_with_same_group_by(get_all_views())))\n",
    "\n",
    "\n",
    "def get_all_combined_queries(views):\n",
    "    \"\"\"Returns a list of all possible queries to be applied after applying the shared-based optimization\n",
    "    Arguments:\n",
    "        views: list of tuples in the form (a, m, f)\"\"\"\n",
    "    queries = []\n",
    "\n",
    "    # Combine Multiple Aggregates\n",
    "    v = get_views_with_same_group_by(views)\n",
    "    for group_by_str, tuples in v.items():\n",
    "        measures = defaultdict(set)\n",
    "        measures_str = []\n",
    "        for (a, m, f) in tuples:\n",
    "            measures[f].add(m)\n",
    "            if m in measures[\"avg\"]:\n",
    "                measures[\"count\"].add(m)\n",
    "        for f in measures:\n",
    "            for m in measures[f]:\n",
    "                measures_str.append(\"{0}({1}) as {0}_{1}\".format(f, m))\n",
    "        combined_measures_str = \", \".join(measures_str)\n",
    "        # Combine reference and target queries\n",
    "        ref_target_string = \"\"\", CASE WHEN marital_status in ('Married-AF-spouse', 'Married-civ-spouse', 'Married-spouse-absent') THEN 'Married' ELSE 'Unmarried' END grouped_marital_status\"\"\"\n",
    "        combined_measures_str += ref_target_string\n",
    "\n",
    "        queries.append(\n",
    "            \"SELECT {1}, {0} FROM census GROUP BY {1}, grouped_marital_status;\".format(\n",
    "                combined_measures_str, group_by_str\n",
    "            )\n",
    "        )\n",
    "    return queries\n",
    "\n",
    "\n",
    "# Test\n",
    "all_views = get_all_views()\n",
    "for i in range(len(dimensions)):\n",
    "    print(get_all_combined_queries(all_views)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mu5KDbnKPb5s",
    "outputId": "aafd737f-36bb-43de-b367-58e2d974613a"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "\n",
    "class Bin:\n",
    "    def __init__(self, c):\n",
    "        self.items = []\n",
    "        self.remaining_cap = c\n",
    "\n",
    "    def add(self, dimension, weight):\n",
    "        self.items.append(dimension)\n",
    "        self.remaining_cap -= weight\n",
    "\n",
    "    def get_dimensions(self):\n",
    "        return self.items\n",
    "\n",
    "    def get_cap(self):\n",
    "        return self.remaining_cap\n",
    "\n",
    "\n",
    "def first_fit(dimensions, c):\n",
    "    bins = []\n",
    "    # add first bin\n",
    "    bins.append(Bin(c))\n",
    "    for dimension, weight in dimensions:\n",
    "        need_new_bin = True\n",
    "        for bin in bins:\n",
    "            if bin.get_cap() >= weight:\n",
    "                bin.add(dimension, weight)\n",
    "                need_new_bin = False\n",
    "                break\n",
    "        if need_new_bin:\n",
    "            bins.append(Bin(c))\n",
    "            bins[-1].add(dimension, weight)\n",
    "\n",
    "    return [b.get_dimensions() for b in bins]\n",
    "\n",
    "\n",
    "def first_fit_dec(dimensions, c):\n",
    "    sorted_dims = sorted(dimensions.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return first_fit(sorted_dims, c)\n",
    "\n",
    "\n",
    "def get_distinct_dimension_counts_log10():\n",
    "    count_str = []\n",
    "    for d in dimensions:\n",
    "        count_str.append(\"count(distinct {0}) as {0}\".format(d))\n",
    "    count_str = \", \".join(count_str)\n",
    "    query = \"SELECT {} FROM census\".format(count_str)\n",
    "    output = execute_query(query)\n",
    "    log_values = np.log10(\n",
    "        np.array(output)[0]\n",
    "    )  # because the output is [[2, 7, 16, 7, 14, 6, 5, 41, 2]]\n",
    "    column_names = output.columns\n",
    "    return dict(zip(column_names, log_values))\n",
    "\n",
    "\n",
    "# Test first_fit_dec\n",
    "\n",
    "dist_dimension_count_log10s = get_distinct_dimension_counts_log10()\n",
    "print(\n",
    "    \"The groups of dimension attributes that we need to combine in GROUP BYs if we have all attributes appearing in views tuples: {}\".format(\n",
    "        first_fit_dec(dist_dimension_count_log10s, np.log10(S_row))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_combined_queries_with_multiple_group_by(views, dist_dimension_count_log10s):\n",
    "    # it takes queries after applying the multiple aggregate and target and reference combining\n",
    "    \"\"\"Returns a list of all possible queries to be applied after applying the shared-based optimization\n",
    "    Arguments:\n",
    "        views: list of tuples in the form (a, m, f)\"\"\"\n",
    "\n",
    "    views_dict = get_views_with_same_group_by(views)\n",
    "    all_dimensions = dist_dimension_count_log10s.keys()\n",
    "    bin_sizes = dist_dimension_count_log10s.copy()\n",
    "    for dimension in all_dimensions:\n",
    "        if dimension not in views_dict.keys():\n",
    "            bin_sizes.pop(dimension)\n",
    "    # dist_dimension_count_log10s holds all the bin sizes for the set of dimension attributes that appear in 'views' tuples\n",
    "\n",
    "    combined_dimensions_list = first_fit_dec(bin_sizes, np.log10(S_row))\n",
    "\n",
    "    queries = []\n",
    "\n",
    "    for combined_dimensions in combined_dimensions_list:\n",
    "        measures = defaultdict(set)\n",
    "        measures_str = []\n",
    "        for dimension in combined_dimensions:\n",
    "            tuples = views_dict[dimension]\n",
    "            for (a, m, f) in tuples:\n",
    "                measures[f].add(m)\n",
    "                if m in measures[\"avg\"]:\n",
    "                    measures[\"count\"].add(m)\n",
    "        for f in measures:\n",
    "            for m in measures[f]:\n",
    "                measures_str.append(\"{0}({1}) as {0}_{1}\".format(f, m))\n",
    "        combined_measures_str = \", \".join(measures_str)\n",
    "        # Combine reference and target queries\n",
    "        ref_target_string = \"\"\", CASE WHEN marital_status in ('Married-AF-spouse', 'Married-civ-spouse', 'Married-spouse-absent') THEN 'Married' ELSE 'Unmarried' END grouped_marital_status\"\"\"\n",
    "        combined_measures_str += ref_target_string\n",
    "        group_by_str = \", \".join(combined_dimensions)\n",
    "        queries.append(\n",
    "            \"SELECT {1}, {0} FROM census GROUP BY {1}, grouped_marital_status;\".format(\n",
    "                combined_measures_str, group_by_str\n",
    "            )\n",
    "        )\n",
    "    return queries\n",
    "\n",
    "\n",
    "# Test\n",
    "all_views = get_all_views()\n",
    "queries = get_all_combined_queries_with_multiple_group_by(\n",
    "    all_views, dist_dimension_count_log10s\n",
    ")\n",
    "for q in queries:\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525
    },
    "id": "JBVbzomuSNVY",
    "outputId": "c04b77d8-c115-4c85-9638-ce4dc7361bf4"
   },
   "outputs": [],
   "source": [
    "def execute_combined_queries_on_partition(combined_queries, part_start, part_end):\n",
    "    results = []\n",
    "    for query in combined_queries:\n",
    "        result_df = run_query_on_partition(query, part_start, part_end)\n",
    "        results.append(result_df)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ICGjwtipJS64"
   },
   "source": [
    "### **Pruning based optimization**\n",
    "\n",
    "Calculate utilities for combined GROUP-BYs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "idSQIouHSMes",
    "outputId": "430f1b83-a50f-4913-ef63-227671682b60"
   },
   "outputs": [],
   "source": [
    "def kl_divergence(target_agg_vals, ref_agg_vals):\n",
    "    # normalize\n",
    "    target_agg_vals = np.array(target_agg_vals, dtype=float)\n",
    "    ref_agg_vals = np.array(ref_agg_vals, dtype=float)\n",
    "    t_sum = sum(target_agg_vals)\n",
    "    r_sum = sum(ref_agg_vals)\n",
    "    t_norm = [i / t_sum for i in target_agg_vals]\n",
    "    r_norm = [i / r_sum for i in ref_agg_vals]\n",
    "    # calculate divergence\n",
    "    return entropy(t_norm, r_norm)\n",
    "\n",
    "\n",
    "def calculate_utility(target_agg_vals, ref_agg_vals):\n",
    "    # Earth Mover's Distance\n",
    "    # return wasserstein_distance(target_agg_vals, ref_agg_vals)\n",
    "    # KL Divergence\n",
    "    return kl_divergence(target_agg_vals, ref_agg_vals)\n",
    "\n",
    "\n",
    "def get_target_and_ref_aggregated_fms(aggregated_fms):\n",
    "    # Now we make sure the same values for dimension attributes exist in the mapping for both target and reference.\n",
    "    # Then, we can return the list of aggregate function values corresponding to those dimension values to be used for calculating utility.\n",
    "    target_fms = aggregated_fms[0]\n",
    "    ref_fms = aggregated_fms[1]\n",
    "    dim_agg_pairs = defaultdict(list)\n",
    "    target_agg_vals = []\n",
    "    ref_agg_vals = []\n",
    "\n",
    "    for dimension_val, agg_val in target_fms.items():\n",
    "        if agg_val == 0:\n",
    "            temp = epsilon\n",
    "        else:\n",
    "            temp = agg_val\n",
    "        dim_agg_pairs[dimension_val].append(temp)\n",
    "\n",
    "    for dimension_val, agg_val in ref_fms.items():\n",
    "        if dimension_val not in dim_agg_pairs:  # dim_val in ref but not in target\n",
    "            dim_agg_pairs[dimension_val].append(epsilon)\n",
    "\n",
    "        if agg_val == 0:\n",
    "            temp = epsilon\n",
    "        else:\n",
    "            temp = agg_val\n",
    "        dim_agg_pairs[dimension_val].append(temp)\n",
    "\n",
    "    for dim_val, agg_vals in dim_agg_pairs.items():\n",
    "        if len(agg_vals) < 2:  # dim_val in target but not in ref\n",
    "            dim_agg_pairs[dim_val].append(epsilon)\n",
    "\n",
    "    for agg_vals in dim_agg_pairs.values():\n",
    "        target_agg_vals.append(agg_vals[0])\n",
    "        ref_agg_vals.append(agg_vals[1])\n",
    "\n",
    "    return target_agg_vals, ref_agg_vals\n",
    "\n",
    "\n",
    "def calculate_dimension_avg(fm_values, count_ms):\n",
    "    agg_val_sum = 0\n",
    "    count_sum = 0\n",
    "    for fm, count in zip(fm_values, count_ms):\n",
    "        count_sum += count\n",
    "        agg_val_sum += fm * count\n",
    "    if count_sum != 0:\n",
    "        return agg_val_sum / count_sum\n",
    "    else:\n",
    "        return epsilon\n",
    "\n",
    "\n",
    "def calculate_aggregate_value(dim_val, fm_values, agg_func, result, m, a):\n",
    "    if agg_func == \"avg\":\n",
    "        count_ms = list(\n",
    "            result[result[a] == dim_val][\"count_{}\".format(m)]\n",
    "        )  # count(m) for the corresponding row\n",
    "        return calculate_dimension_avg(count_ms, fm_values)\n",
    "    elif agg_func == \"min\":\n",
    "        return min(fm_values)\n",
    "    elif agg_func == \"max\":\n",
    "        return max(fm_values)\n",
    "    elif agg_func == \"sum\":\n",
    "        return sum(fm_values)\n",
    "    elif agg_func == \"count\":\n",
    "        return sum(fm_values)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Aggregate function cannot be {} for dimension value {}!\".format(\n",
    "                agg_func, dim_val\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "def calc_utilities(results, views=get_all_views()):\n",
    "    utilities = {}\n",
    "    aggregated_fms = [defaultdict(list), defaultdict(list)]\n",
    "    for (a, m, f) in views:\n",
    "        for i in range(len(results)):  # for target and reference results\n",
    "            result = results[i]\n",
    "            for dim_val in result[a].unique():  # for every value of a dimension\n",
    "                # if dim_val == '?':\n",
    "                #     continue\n",
    "                fm_values = list(\n",
    "                    result[result[a] == dim_val][\"{}_{}\".format(f, m)]\n",
    "                )  # get all of corresponding f(m) values\n",
    "                aggregated_fms[i][dim_val] = calculate_aggregate_value(\n",
    "                    dim_val, fm_values, f, result, m, a\n",
    "                )\n",
    "                target_agg_vals, ref_agg_vals = get_target_and_ref_aggregated_fms(\n",
    "                    aggregated_fms\n",
    "                )\n",
    "        utilities[(a, m, f)] = calculate_utility(target_agg_vals, ref_agg_vals)\n",
    "    return utilities\n",
    "\n",
    "\n",
    "def calc_utilities_given_views(\n",
    "    target_results, reference_results, views=get_all_views()\n",
    "):\n",
    "    utilities = {}\n",
    "    aggregated_fms = [defaultdict(list), defaultdict(list)]\n",
    "    all_query_results = [target_results, reference_results]\n",
    "    for (a, m, f) in views:\n",
    "        for i in range(len(all_query_results)):  # for target and reference results\n",
    "            result = None\n",
    "            for query_results in all_query_results[i]:\n",
    "                if set((a, \"{}_{}\".format(f, m))).issubset(query_results.columns):\n",
    "                    result = query_results\n",
    "\n",
    "            for dim_val in result[a].unique():  # for every value of a dimension\n",
    "                # if dim_val == '?':\n",
    "                #     continue\n",
    "                fm_values = list(\n",
    "                    result[result[a] == dim_val][\"{}_{}\".format(f, m)]\n",
    "                )  # get all of corresponding f(m) values\n",
    "                aggregated_fms[i][dim_val] = calculate_aggregate_value(\n",
    "                    dim_val, fm_values, f, result, m, a\n",
    "                )\n",
    "                target_agg_vals, ref_agg_vals = get_target_and_ref_aggregated_fms(\n",
    "                    aggregated_fms\n",
    "                )\n",
    "        utilities[(a, m, f)] = calculate_utility(target_agg_vals, ref_agg_vals)\n",
    "    return utilities\n",
    "\n",
    "\n",
    "# view_utilities = calc_utilities(results[0:2]) # we should change here Hadeel\n",
    "# view_utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hoeffding_Serfling_Error(m, N, delta=10**-2):\n",
    "    return math.sqrt(\n",
    "        (1 - (m - 1) / N)\n",
    "        * (2 * math.log(math.log(m)) + math.log(math.pi**2 / (3 * delta)))\n",
    "        / (2 * m)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval_pruning(upperbound, lowerbound, optimized_queries, k):\n",
    "    top_k_queries = set(heapq.nlargest(k, upperbound, key=upperbound.get))\n",
    "    top_k_lowest_lowerbound = min(lowerbound[q] for q in top_k_queries)\n",
    "    for query in optimized_queries:\n",
    "        if query not in top_k_queries:\n",
    "            if upperbound[query] < top_k_lowest_lowerbound:\n",
    "                optimized_queries.remove(query)\n",
    "    return optimized_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAB_pruning(current_mean, optimized_queries, k):\n",
    "    top_k_1 = heapq.nlargest(k + 1, current_mean, key=current_mean.get)\n",
    "    sorted_queries = sorted(\n",
    "        list(optimized_queries), key=lambda q: current_mean[q], reverse=True\n",
    "    )\n",
    "    lowest = min(current_mean[q] for q in current_mean)\n",
    "    delta_1 = top_k_1[0][1] - top_k_1[k - 1][1]\n",
    "    delta_n = top_k_1[-1][1] - lowest\n",
    "    if delta_1 > delta_n:\n",
    "        sorted_queries.pop(0)\n",
    "    else:\n",
    "        sorted_queries.pop(-1)\n",
    "    return set(sorted_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_views(m, top_views, running_mean, prune_method=\"CI\", N=n_partitions, k=5):\n",
    "\n",
    "    # print(\"prune started\")\n",
    "    if m == 1:\n",
    "        return top_views\n",
    "    if prune_method == \"CI\":\n",
    "        upperbounds, lowerbounds = {}, {}\n",
    "        confidence_interval = Hoeffding_Serfling_Error(m, N)\n",
    "        for q in top_views:\n",
    "            upperbounds[q] = running_mean[q] + confidence_interval\n",
    "            lowerbounds[q] = running_mean[q] - confidence_interval\n",
    "        return confidence_interval_pruning(upperbounds, lowerbounds, top_views, k)\n",
    "    elif prune_method == \"MAB\":\n",
    "        return MAB_pruning(running_mean, top_views, k)\n",
    "    else:\n",
    "        return top_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CPMzlCc-fgI"
   },
   "source": [
    "### **Visualizing top k queries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_views(view_utilities, k):\n",
    "    return sorted(view_utilities.items(), key=lambda item: item[1], reverse=True)[:k]\n",
    "\n",
    "\n",
    "# get_top_k_views(view_utilities, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1oCvqdW-eLw"
   },
   "outputs": [],
   "source": [
    "# This list is used only for testing, it should be removed\n",
    "views_list = [\n",
    "    (\"sex\", \"capital_gain\", \"avg\"),\n",
    "    (\"sex\", \"age\", \"avg\"),\n",
    "    (\"native_country\", \"capital_loss\", \"max\")\n",
    "]\n",
    "\n",
    "#plt.rcParams[\"figure.figsize\"] = (20, 8)\n",
    "plt.rc(\"xtick\", labelsize=10)\n",
    "plt.rc(\"ytick\", labelsize=14)\n",
    "\n",
    "\n",
    "def draw_views(views_list):\n",
    "    for view in views_list:\n",
    "        group_by, measurement, agg_func = view\n",
    "        # if group_by == \"marital_status\":\n",
    "        #     query = \"\"\"select CASE WHEN marital_status in ('Married-AF-spouse', 'Married-civ-spouse', 'Married-spouse-absent') THEN 'Married' ELSE 'Unmarried' END grouped_marital_status\n",
    "        #     , {}({}) from census Group by grouped_marital_status;\"\"\".format(\n",
    "        #         agg_func, measurement\n",
    "        #     )\n",
    "        #     group_by = \"grouped_marital_status\"\n",
    "        #     res = execute_query(query)\n",
    "        #     target_df = res.loc[res[\"grouped_marital_status\"] == \"Married\"].reset_index(\n",
    "        #         drop=True\n",
    "        #     )\n",
    "        #     ref_df = res.loc[res[\"grouped_marital_status\"] == \"Unmarried\"].reset_index(\n",
    "        #         drop=True\n",
    "        #     )\n",
    "\n",
    "        # else:\n",
    "        target_query = \"select {0}, {1}({2}) from census where marital_status in ('Married-AF-spouse', 'Married-civ-spouse', 'Married-spouse-absent') Group By {0};\".format(\n",
    "            group_by, agg_func, measurement\n",
    "        )\n",
    "        ref_query = \"select {0}, {1}({2}) from census where marital_status in ('Never-married', 'Widowed','Divorced', 'Separated') Group By {0};\".format(\n",
    "            group_by, agg_func, measurement\n",
    "        )\n",
    "        target_df = execute_query(target_query)\n",
    "        ref_df = execute_query(ref_query)\n",
    "        \n",
    "        \n",
    "        # populate both df with the same values for the groups so target and ref have same groups\n",
    "        for t in target_df[group_by]:\n",
    "            if t not in list(ref_df[group_by]):\n",
    "                ref_df.loc[len(ref_df.index)] = [t, 0]\n",
    "        for r in ref_df[group_by]:\n",
    "            if r not in list(target_df[group_by]):\n",
    "                target_df.loc[len(target_df.index)] = [r, 0]\n",
    "        target_df = target_df.sort_values(by=[group_by])\n",
    "        ref_df = ref_df.sort_values(by=[group_by])\n",
    "\n",
    "        indices = list(target_df.columns)\n",
    "        target_val = target_df[indices[1]]\n",
    "        ref_val = ref_df[indices[1]]\n",
    "\n",
    "        # create plot\n",
    "        n_groups = len(target_df)\n",
    "        fig, ax = plt.subplots()\n",
    "        index = np.arange(n_groups)\n",
    "        bar_width = 0.15\n",
    "        opacity = 0.8\n",
    "\n",
    "        rects1 = plt.bar(\n",
    "            index, # - 0.5 * bar_width,\n",
    "            ref_val,\n",
    "            bar_width,\n",
    "            alpha=opacity,\n",
    "            color=\"c\",\n",
    "            label=\"unmarried\",\n",
    "        )\n",
    "        rects2 = plt.bar(\n",
    "            index + bar_width,\n",
    "            target_val,\n",
    "            bar_width,\n",
    "            alpha=opacity,\n",
    "            color=\"b\",\n",
    "            label=\"married\",\n",
    "        )\n",
    "\n",
    "        if(group_by == 'native_country'):\n",
    "            fig.set_size_inches(30, 8)\n",
    "        else:\n",
    "            fig.set_size_inches(8, 6)\n",
    "        \n",
    "\n",
    "        plt.xlabel(\"{}\".format(group_by), fontsize=16)\n",
    "        plt.ylabel(\"{}({})\".format(agg_func, measurement), fontsize=16)\n",
    "\n",
    "        plt.xticks(index + 0.5*bar_width, ref_df[indices[0]], rotation=45)\n",
    "        plt.legend(fontsize=14)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        figname = \"{0}({1}) {2}.png\".format(agg_func, measurement, group_by)\n",
    "        plt.savefig(figname)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# draw_views(views_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_algorithms(apply_multiple_group_by, apply_pruning, db_partitions=10, k=5):\n",
    "    # partition the db\n",
    "    all_partitions_start = partition_DB(db_partitions)\n",
    "\n",
    "    # The execution engine begins with the entire set of aggregate views under consideration.\n",
    "    top_views = get_all_views()\n",
    "    running_mean = defaultdict(float)\n",
    "    dist_dimension_count_log10s = get_distinct_dimension_counts_log10()\n",
    "    for i in range(db_partitions):\n",
    "        # print(\"Phase {} start:\".format(i))\n",
    "\n",
    "        # During phase i, SeeDB updates partial results for the views still under consideration using the ith partition of the dataset.\n",
    "        part_start = all_partitions_start[i]\n",
    "        part_end = all_partitions_start[i + 1]\n",
    "\n",
    "        # The execution engine applies sharing-based optimizations to minimize scans on the ith partition of the dataset.\n",
    "        # combined queries will start with the entire set of aggregate views under consideration.\n",
    "        if not apply_multiple_group_by:\n",
    "            combined_queries = get_all_combined_queries(top_views)\n",
    "        else:\n",
    "            combined_queries = get_all_combined_queries_with_multiple_group_by(\n",
    "                top_views, dist_dimension_count_log10s\n",
    "            )\n",
    "\n",
    "        target_view_data = []\n",
    "        reference_view_data = []\n",
    "\n",
    "        results = execute_combined_queries_on_partition(\n",
    "            combined_queries, part_start, part_end\n",
    "        )\n",
    "\n",
    "        # each item in results will contain a dataframe where the rows are the result\n",
    "        # of executing the combined query on the database which inlucde both reference and target data\n",
    "        for result in results:\n",
    "            target_view_data.append(\n",
    "                result.loc[result[\"grouped_marital_status\"] == \"Married\"]\n",
    "            )\n",
    "            reference_view_data.append(\n",
    "                result.loc[result[\"grouped_marital_status\"] == \"Unmarried\"]\n",
    "            )\n",
    "\n",
    "        # Calcuate the utility scores and the running average of the scores for each running query\n",
    "        utilities = calc_utilities_given_views(\n",
    "            target_view_data, reference_view_data, top_views\n",
    "        )\n",
    "        for q in top_views:\n",
    "            running_mean[q] = (running_mean[q] * (i) + utilities[q]) / (i + 1)\n",
    "\n",
    "        # At the end of phase i, the execution engine uses pruning-based optimizations to determine which aggregate views to discard.\n",
    "        if apply_pruning:\n",
    "            top_views = prune_views(i + 1, top_views, running_mean, k)\n",
    "            # print(\"Remaining views:\", len(top_views))\n",
    "        # print(\"Current top k:\", get_top_k_views(running_mean, 5))\n",
    "\n",
    "    # #draw the top k views, printing only 3 for testin\n",
    "    top_k = get_top_k_views(running_mean, k)\n",
    "    #print(top_k)\n",
    "    top_k_views = [x[0] for x in top_k]\n",
    "    # draw_views(top_k_views)\n",
    "    return top_k_views\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#No partitioning, Applying the basic algorithm with only grouping measure attributes\n",
    "v = apply_algorithms(apply_multiple_group_by=False, apply_pruning=False, db_partitions=1, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v)\n",
    "draw_views(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "apply_algorithms(apply_multiple_group_by=False, apply_pruning=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "v = apply_algorithms(apply_multiple_group_by=True, apply_pruning=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9I6iMBkEI4et",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(v)\n",
    "draw_views(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "v = apply_algorithms(apply_multiple_group_by=True, apply_pruning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v)\n",
    "draw_views(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in [10, 100, 1000, 10000, 100000, 1000000, 10000000]:\n",
    "    S_row = s\n",
    "    print(\n",
    "        \"S_row = {} took {} seconds.\".format(\n",
    "            s, timeit.timeit(lambda: apply_algorithms(False, True), number=20)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure time with pruning and combining GROUP BYs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in [10, 100, 1000, 10000, 100000, 1000000, 10000000]:\n",
    "    S_row = s\n",
    "    print(\n",
    "        \"S_row = {} took {} seconds.\".format(\n",
    "            s, timeit.timeit(lambda: apply_algorithms(True, True), number=20)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: create a separate cell for generating the plots and saving them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOv-iLOMI_WQ"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# n_splits = 10\n",
    "# split_path = Path('/content/census.splits')\n",
    "# split_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# df = pd.DataFrame(pd.read_csv('/content/adultdata.csv'))\n",
    "# df.columns = [i.replace('-', '_') for i in df.columns]\n",
    "\n",
    "# df_split= np.array_split(df, n_splits)\n",
    "\n",
    "# for i in range(len(df_split)):\n",
    "#     df_split[i].to_csv(Path(split_path / \"census.split_{}.csv\".format(i)), encoding='utf-8', index=False)\n",
    "\n",
    "# !zip -r /content/census.splits.zip /content/census.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khbzkeo-vENs"
   },
   "outputs": [],
   "source": [
    "# for i in range(n_splits):\n",
    "#     cur.executescript('''drop table if exists split_{};\n",
    "#                         create table split_{}\n",
    "#                         (age int, work_class text, fnlwgt int, education text, education_num int, marital_status text, occupation text, relationship text, \n",
    "#                         race text, sex text, capital_gain int, capital_loss int, hours_per_week int, native_country text, class text)'''.format(i, i))\n",
    "    \n",
    "#     # change this to psql\n",
    "#     df = pd.DataFrame(pd.read_csv('/content/census.splits/census.split_{}.csv'.format(i)))\n",
    "#     for row in df.itertuples():\n",
    "#         cur.execute('''\n",
    "#                   INSERT INTO split_{} (age, work_class, fnlwgt, education, education_num, marital_status, occupation, relationship, \n",
    "#                   race, sex, capital_gain, capital_loss, hours_per_week, native_country, class)\n",
    "#                   VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)\n",
    "#                   '''.format(i),\n",
    "#                   (row.AGE, row.WORK_CLASS, row.FNLWGT, row.EDUCATION, row.EDUCATION_NUM, row.MARITAL_STATUS, row.OCCUPATION, row.RELATIONSHIP, \n",
    "#                   row.RACE, row.SEX, row.CAPITAL_GAIN, row.CAPITAL_LOSS, row.HOURS_PER_WEEK, row.NATIVE_COUNTRY, row.CLASS)\n",
    "#                   )   \n",
    "\n",
    "# conn.commit()\n",
    "# census_view_query = 'drop view if exists census_view; create view census_view as select * from split_0'\n",
    "# for i in range(1, n_splits):\n",
    "#     census_view_query += ' union select * from split_{}'.format(i)\n",
    "# census_view_query += ';'\n",
    "# cur.executescript(census_view_query)\n",
    "# conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8bc_6_avMfY"
   },
   "outputs": [],
   "source": [
    "# n_phases = n_splits\n",
    "# for i in range(n_phases):\n",
    "#     query = \"\"\"drop view if exists split_{0}_married; create view split_{0}_married as select * from split_{0} where marital_status in ('Married-AF-spouse', 'Married-civ-spouse', 'Married-spouse-absent','Separated');\n",
    "#            drop view if exists split_{0}_unmarried; create view split_{0}_unmarried as select * from split_{0} where marital_status in ('Never-married', 'Widowed','Divorced');\"\"\".format(i)\n",
    "#     cur.executescript(query)\n",
    "#     conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SeeDB.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
